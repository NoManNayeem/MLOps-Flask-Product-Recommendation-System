{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJOPOJg3Owv0"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Generating the Synthetic Dataset\n",
        "\n",
        "We'll generate synthetic data in three parts:\n",
        "\n",
        "- User Data: Simulate user demographic details.\n",
        "- Product Data: Simulate product details.\n",
        "- Interaction Data: Simulate how users interact with products (e.g., purchase or rate)."
      ],
      "metadata": {
        "id": "3fTgU9fbQYZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # To handle data in tabular form\n",
        "import numpy as np   # To generate random data\n",
        "\n",
        "# Step 1: Define the number of users and products\n",
        "# Let's assume we have 1000 users and 500 products in our ecommerce platform.\n",
        "num_users = 1000\n",
        "num_products = 500\n",
        "\n",
        "# Step 2: Generating the Users Data\n",
        "# Each user has an ID, age, gender, and location.\n",
        "user_data = {\n",
        "    'user_id': np.arange(1, num_users + 1),  # Generate user IDs from 1 to 1000\n",
        "    'age': np.random.randint(18, 70, size=num_users),  # Random ages between 18 and 70\n",
        "    'gender': np.random.choice(['M', 'F'], size=num_users),  # Randomly assign gender as Male (M) or Female (F)\n",
        "    'location': np.random.choice(['Urban', 'Suburban', 'Rural'], size=num_users)  # Randomly assign location type\n",
        "}\n",
        "\n",
        "# Convert the user data dictionary into a pandas DataFrame\n",
        "users_df = pd.DataFrame(user_data)\n",
        "\n",
        "# Step 3: Generating the Products Data\n",
        "# Each product has an ID, category, price, and rating.\n",
        "product_data = {\n",
        "    'product_id': np.arange(1, num_products + 1),  # Generate product IDs from 1 to 500\n",
        "    'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books'], size=num_products),  # Randomly assign product category\n",
        "    'price': np.round(np.random.uniform(5, 500, size=num_products), 2),  # Random prices between $5 and $500, rounded to 2 decimal places\n",
        "    'rating': np.round(np.random.uniform(1, 5, size=num_products), 1)  # Random ratings between 1 and 5, rounded to 1 decimal place\n",
        "}\n",
        "\n",
        "# Convert the product data dictionary into a pandas DataFrame\n",
        "products_df = pd.DataFrame(product_data)\n",
        "\n",
        "# Step 4: Generating the User-Product Interaction Data (Purchase History or Ratings)\n",
        "# We simulate how users interact with products. For example, users can rate or buy products.\n",
        "\n",
        "interaction_data = {\n",
        "    'user_id': np.random.choice(users_df['user_id'], size=5000),  # Randomly select users who interacted with products\n",
        "    'product_id': np.random.choice(products_df['product_id'], size=5000),  # Randomly select products that were interacted with\n",
        "    'rating': np.random.randint(1, 6, size=5000),  # Assign random ratings (1 to 5 stars) for these interactions\n",
        "    'timestamp': pd.date_range(start='2023-01-01', periods=5000, freq='T')  # Generate random timestamps for interactions, 1 minute apart\n",
        "}\n",
        "\n",
        "# Convert the interaction data dictionary into a pandas DataFrame\n",
        "interactions_df = pd.DataFrame(interaction_data)\n",
        "\n",
        "# Let's check the first few rows of each dataset\n",
        "users_df.head(), products_df.head(), interactions_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6OUrb2gQnX4",
        "outputId": "0acf3b48-34c9-44d5-ebf5-d570fb3d6a4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   user_id  age gender  location\n",
              " 0        1   30      M     Rural\n",
              " 1        2   34      M     Urban\n",
              " 2        3   30      M  Suburban\n",
              " 3        4   45      M  Suburban\n",
              " 4        5   44      M  Suburban,\n",
              "    product_id     category   price  rating\n",
              " 0           1        Books  329.69     4.7\n",
              " 1           2  Electronics   57.92     3.2\n",
              " 2           3     Clothing  201.51     2.2\n",
              " 3           4     Clothing   46.14     2.7\n",
              " 4           5        Books  396.72     2.4,\n",
              "    user_id  product_id  rating           timestamp\n",
              " 0      166         148       3 2023-01-01 00:00:00\n",
              " 1      866         211       4 2023-01-01 00:01:00\n",
              " 2        7         193       3 2023-01-01 00:02:00\n",
              " 3      423          75       3 2023-01-01 00:03:00\n",
              " 4      478         419       2 2023-01-01 00:04:00)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "1. Libraries:\n",
        "\n",
        "- pandas: Used for handling tabular data (like spreadsheets).\n",
        "numpy: Helps generate random values for simulating user, product, and interaction data.\n",
        "\n",
        "2. User Data:\n",
        "\n",
        "- We create 1000 users with random ages, genders (male/female), and locations (urban/suburban/rural).\n",
        "3. Product Data:\n",
        "\n",
        "- We create 500 products, each belonging to a random category (Electronics, Clothing, Home, Books), with random prices and ratings.\n",
        "\n",
        "4. Interaction Data:\n",
        "\n",
        "- We simulate 5000 interactions between users and products, each with a rating (1 to 5 stars) and a timestamp to capture when the interaction happened."
      ],
      "metadata": {
        "id": "YfZB0nkpQtGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Pre-processing.\n",
        "\n",
        "Data pre-processing involves preparing the data for training by handling missing values, encoding categorical data, and normalizing or scaling numerical data, if needed. In the context of a recommendation system, we mainly focus on:\n",
        "\n",
        "1. Handling missing values: Ensure no missing data in users, products, or interactions.\n",
        "2. Encoding categorical variables: Convert categories like gender or category into numerical format.\n",
        "3. Creating a user-product matrix: This will be the input for our recommendation model, where each row represents a user, each column represents a product, and the cells represent ratings or interactions."
      ],
      "metadata": {
        "id": "w5B2GTBRRG0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for pre-processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# Checking for missing values in all datasets\n",
        "print(\"Missing values in users data:\\n\", users_df.isnull().sum())\n",
        "print(\"Missing values in products data:\\n\", products_df.isnull().sum())\n",
        "print(\"Missing values in interactions data:\\n\", interactions_df.isnull().sum())\n",
        "\n",
        "# If there were any missing values, we would handle them here. Since this is synthetic data, it’s unlikely.\n",
        "\n",
        "# Step 2: Encoding categorical variables\n",
        "# We need to convert categorical variables like 'gender' and 'category' into numerical format.\n",
        "# Using LabelEncoder to encode these categorical features\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the gender column in users data (M -> 0, F -> 1)\n",
        "users_df['gender_encoded'] = label_encoder.fit_transform(users_df['gender'])\n",
        "\n",
        "# Encode the location column in users data\n",
        "users_df['location_encoded'] = label_encoder.fit_transform(users_df['location'])\n",
        "\n",
        "# Encode the category column in products data\n",
        "products_df['category_encoded'] = label_encoder.fit_transform(products_df['category'])\n",
        "\n",
        "# Step 3: Create a User-Product Rating Matrix\n",
        "# We pivot the interactions data to create a matrix with users as rows, products as columns, and ratings as values.\n",
        "user_product_matrix = interactions_df.pivot_table(index='user_id', columns='product_id', values='rating').fillna(0)\n",
        "\n",
        "# Step 4: Train-test split\n",
        "# We will split the user-product interaction data into training and test sets.\n",
        "train_data, test_data = train_test_split(interactions_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Let's display the first few rows of the pre-processed data to verify\n",
        "print(\"User-Product Matrix:\\n\", user_product_matrix.head())\n",
        "print(\"Train Data Sample:\\n\", train_data.head())\n",
        "print(\"Test Data Sample:\\n\", test_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8m1BQqAQosb",
        "outputId": "b148560c-3483-453d-9daa-9f13991daa76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in users data:\n",
            " user_id     0\n",
            "age         0\n",
            "gender      0\n",
            "location    0\n",
            "dtype: int64\n",
            "Missing values in products data:\n",
            " product_id    0\n",
            "category      0\n",
            "price         0\n",
            "rating        0\n",
            "dtype: int64\n",
            "Missing values in interactions data:\n",
            " user_id       0\n",
            "product_id    0\n",
            "rating        0\n",
            "timestamp     0\n",
            "dtype: int64\n",
            "User-Product Matrix:\n",
            " product_id  1    2    3    4    5    6    7    8    9    10   ...  491  492  \\\n",
            "user_id                                                       ...             \n",
            "1           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
            "2           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
            "3           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
            "4           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
            "5           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0   \n",
            "\n",
            "product_id  493  494  495  496  497  498  499  500  \n",
            "user_id                                             \n",
            "1           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "2           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "3           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "4           0.0  0.0  0.0  4.5  0.0  0.0  0.0  0.0  \n",
            "5           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "\n",
            "[5 rows x 500 columns]\n",
            "Train Data Sample:\n",
            "       user_id  product_id  rating           timestamp\n",
            "4227      698         496       4 2023-01-03 22:27:00\n",
            "4676      574         253       2 2023-01-04 05:56:00\n",
            "800       333         398       4 2023-01-01 13:20:00\n",
            "3671      104         122       4 2023-01-03 13:11:00\n",
            "4193      253         228       4 2023-01-03 21:53:00\n",
            "Test Data Sample:\n",
            "       user_id  product_id  rating           timestamp\n",
            "1501      528         100       1 2023-01-02 01:01:00\n",
            "2586      840         210       4 2023-01-02 19:06:00\n",
            "2653      670         410       4 2023-01-02 20:13:00\n",
            "1055      854         238       4 2023-01-01 17:35:00\n",
            "705       889         442       2 2023-01-01 11:45:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "1. Handling Missing Values:\n",
        "\n",
        "- We check for missing values using isnull().sum(). Since our data is synthetic, there should be no missing values, but in real-world data, you'd handle this by filling missing values or dropping them.\n",
        "\n",
        "2. Encoding Categorical Variables:\n",
        "\n",
        "- We convert categorical variables such as gender, location, and category into numerical format using LabelEncoder, which is important for machine learning models to understand non-numerical data.\n",
        "\n",
        "3. Creating User-Product Matrix:\n",
        "\n",
        "- We use a pivot table to transform the interactions_df into a matrix where rows represent users, columns represent products, and values represent the rating. This matrix is what we’ll use to train our recommendation model.\n",
        "\n",
        "4. Train-test Split:\n",
        "\n",
        "- We split the interaction data into training (80%) and testing (20%) sets to evaluate our model later on.\n"
      ],
      "metadata": {
        "id": "UDnblceNRVb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Testing.\n",
        "\n",
        "We’ll implement a Collaborative Filtering recommendation system using the Surprise library, which is popular for building recommendation systems. The algorithm we’ll use is Singular Value Decomposition (SVD), a matrix factorization technique commonly used for collaborative filtering.\n",
        "\n",
        "Here’s what we’ll do in this step:\n",
        "\n",
        "- Install and Import the Surprise library.\n",
        "- Prepare the data for the Surprise library.\n",
        "- Train the SVD model using the training dataset.\n",
        "- Evaluate the model on the test dataset using metrics such as RMSE (Root Mean Squared Error)."
      ],
      "metadata": {
        "id": "dCGS1W5JRo-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Surprise library\n",
        "!pip install scikit-surprise\n",
        "\n",
        "# Import necessary libraries\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise import accuracy\n",
        "\n",
        "# Step 1: Prepare the data for Surprise\n",
        "# Surprise expects data to have 3 columns: user_id, product_id, and rating. We'll use the interaction data for this.\n",
        "\n",
        "reader = Reader(rating_scale=(1, 5))  # The rating scale in our dataset is from 1 to 5\n",
        "data = Dataset.load_from_df(interactions_df[['user_id', 'product_id', 'rating']], reader)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "# We perform a 80/20 train-test split on the Surprise dataset\n",
        "trainset, testset = surprise_train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Step 3: Train the SVD model\n",
        "# SVD is a matrix factorization technique used for collaborative filtering\n",
        "model = SVD()  # Initialize the SVD model\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(trainset)\n",
        "\n",
        "# Step 4: Test the model on the test set\n",
        "# We predict the ratings for the test set and evaluate performance\n",
        "predictions = model.test(testset)\n",
        "\n",
        "# Step 5: Evaluate the performance using RMSE\n",
        "rmse = accuracy.rmse(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqD8Su8aRQy5",
        "outputId": "c746ca9d-cc7a-4f08-b390-83a7d2b79731"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357281 sha256=1b9d346a0779f18099d11039841820b0c8708c9f7628d6f890349a07d7993e4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n",
            "RMSE: 1.4730\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "1. Installing the Surprise Library:\n",
        "\n",
        "- We use the Surprise library, which is specifically designed for building recommendation systems. It supports various algorithms, including SVD, KNN, and more.\n",
        "\n",
        "2. Preparing Data:\n",
        "\n",
        "- We format the interaction data (user_id, product_id, rating) in a way that the Surprise library understands using the Reader class.\n",
        "\n",
        "3. Train-test Split:\n",
        "\n",
        "- We split the data into training and test sets (80/20 split) using surprise_train_test_split().\n",
        "\n",
        "4. Training the SVD Model:\n",
        "\n",
        "- We initialize the SVD model and train it on the training set. The SVD algorithm performs matrix factorization, which is suitable for collaborative filtering tasks.\n",
        "\n",
        "5. Evaluating the Model:\n",
        "\n",
        "- After training the model, we test it on the test set. We calculate the RMSE (Root Mean Squared Error), which tells us how well the model is predicting user ratings.\n",
        "\n",
        "**The RMSE (Root Mean Squared Error) of your model is 1.4730, which indicates the average error between the predicted and actual ratings. This score is a good starting point, but there's room for improvement by tuning the model or using other techniques like incorporating more data or trying different algorithms.**"
      ],
      "metadata": {
        "id": "AWs8qZDMR0GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Saving the Model\n",
        "- We will use Python's pickle library to serialize (save) the model to a file. This way, you can load the model in your Flask app and use it to make predictions.\n"
      ],
      "metadata": {
        "id": "zO49sCGVSV2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Step 1: Save the trained SVD model to a file\n",
        "model_filename = 'svd_model.pkl'\n",
        "with open(model_filename, 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "print(f\"Model saved to {model_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSuC5wK8SVWM",
        "outputId": "264ad0c1-6be6-49a1-848a-88f25bae05d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to svd_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "1. Pickle Library: This is used to serialize the Python objects (in this case, the trained model) into a file.\n",
        "2. Saving the Model: We open a file in write-binary mode (wb) and save the model to svd_model.pkl."
      ],
      "metadata": {
        "id": "Vj2f02HOSeeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Step: Download the Model File\n",
        "- After saving the model, we can use google.colab.files to download the file directly to your computer."
      ],
      "metadata": {
        "id": "u5fPH6YeSpz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the saved model file\n",
        "files.download(model_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "M6sJJjKPRwQ9",
        "outputId": "b0f5dcb0-8556-498e-c2db-3672169ae70e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_94a2dd4d-49c5-4bec-b2e6-b771cb47b1c9\", \"svd_model.pkl\", 1325582)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "- files.download(): This method from the google.colab package allows you to download any file from the Colab environment to your local machine."
      ],
      "metadata": {
        "id": "NJcu_W_BS0fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Monitoring the Model Performance\n",
        "- Let’s extend the RMSE evaluation by calculating MAE (Mean Absolute Error) and generating a performance report."
      ],
      "metadata": {
        "id": "tD1IihAMS_mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate MAE (Mean Absolute Error)\n",
        "mae = accuracy.mae(predictions)\n",
        "\n",
        "# Step 2: Generate a basic performance report\n",
        "performance_report = {\n",
        "    'RMSE': rmse,\n",
        "    'MAE': mae\n",
        "}\n",
        "\n",
        "# Display the performance report\n",
        "print(\"Model Performance Report:\")\n",
        "for metric, score in performance_report.items():\n",
        "    print(f\"{metric}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkf3goSPSuph",
        "outputId": "48b31a6a-01ae-4041-b9bb-76ef5b6f0b99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE:  1.2755\n",
            "Model Performance Report:\n",
            "RMSE: 1.4730\n",
            "MAE: 1.2755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "1. MAE (Mean Absolute Error):\n",
        "\n",
        "- While RMSE penalizes larger errors more, MAE gives an average of absolute errors. Both metrics are useful for understanding the model's performance.\n",
        "\n",
        "2. Performance Report:\n",
        "\n",
        "- We store and print both RMSE and MAE, which gives you an overview of how the model is performing.\n",
        "\n",
        "# Additional Step: Advanced Monitoring (Optional)\n",
        "If you want to go beyond basic monitoring, you can:\n",
        "\n",
        "1. Track performance over time: You can log the performance after every training cycle to see if the model improves or worsens.\n",
        "2. Visualization: You can create plots showing error distribution, or monitor the recommendation accuracy using recall/precision if you have ground truth data."
      ],
      "metadata": {
        "id": "xCxhv35ATF7h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bkOiaKBZTDsp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}